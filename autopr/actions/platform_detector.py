"""
AutoPR Action: Platform Detector
Detects which rapid prototyping platform was used and routes accordingly
"""

import json
import os
import re
from typing import Dict, List, Any, Optional, Tuple
from pydantic import BaseModel
from pathlib import Path

class PlatformDetectorInputs(BaseModel):
    repository_url: str
    commit_messages: List[str] = []
    workspace_path: str = "."
    package_json_content: Optional[str] = None

class PlatformDetectorOutputs(BaseModel):
    detected_platform: str  # "replit", "lovable", "bolt", "same", "emergent", "unknown"
    confidence_score: float
    platform_specific_config: Dict[str, Any]
    recommended_workflow: str
    migration_suggestions: List[str]
    enhancement_opportunities: List[str]

class PlatformDetector:
    def __init__(self) -> None:
        self.platform_signatures: Dict[str, Dict[str, Any]] = {
            'replit': {
                'files': ['.replit', 'replit.nix', 'pyproject.toml', '.replit.json'],
                'package_scripts': ['repl-run', 'replit-dev', 'repl-dev'],
                'commit_patterns': ['replit', 'exported from replit', 'repl.it', 'from replit'],
                'dependencies': ['@replit/database', 'replit-py'],
                'folder_patterns': ['.replit_modules', 'replit_modules'],
                'content_patterns': ['replit.com', 'repl.co']
            },
            'lovable': {
                'files': ['lovable.config.js', '.lovable', 'lovable.json'],
                'dependencies': ['@lovable/core', 'lovable-ui', '@lovable/cli'],
                'commit_patterns': ['lovable', 'generated by lovable', 'lovable.dev'],
                'package_scripts': ['lovable:dev', 'lovable:build', 'lovable:deploy'],
                'content_patterns': ['lovable.dev', 'generated with lovable']
            },
            'bolt': {
                'files': ['bolt.config.json', '.bolt', 'bolt.json', '.bolt.config'],
                'package_scripts': ['bolt:dev', 'bolt:build', 'bolt:start'],
                'commit_patterns': ['bolt.new', 'bolt generated', 'created with bolt'],
                'dependencies': ['@bolt/core', 'bolt-cli'],
                'content_patterns': ['bolt.new', 'built with bolt']
            },
            'same': {
                'files': ['same.config.js', '.same', 'same.json'],
                'commit_patterns': ['same.new', 'cloned from same', 'same template'],
                'dependencies': ['@same/core', 'same-cli'],
                'package_scripts': ['same:dev', 'same:clone', 'same:customize'],
                'content_patterns': ['same.new', 'cloned with same']
            },
            'emergent': {
                'files': ['emergent.sh', '.emergent', 'deploy.emergent', '.emergent.yml'],
                'commit_patterns': ['emergent.sh', 'automated by emergent', 'emergent deploy'],
                'content_patterns': ['emergent.sh', 'emergent automation'],
                'folder_patterns': ['emergent_scripts', '.emergent_cache']
            }
        }
        
        # Enhanced detection patterns for better accuracy
        self.advanced_patterns: Dict[str, Dict[str, Any]] = {
            'replit': {
                'readme_patterns': ['run on replit', 'replit button', 'repl.it'],
                'env_patterns': ['REPL_ID', 'REPL_OWNER', 'REPL_SLUG'],
                'dockerfile_patterns': ['replit/polygott', 'replit run']
            },
            'lovable': {
                'readme_patterns': ['built with lovable', 'lovable.dev'],
                'component_patterns': ['lovable-component', '@lovable/']
            },
            'bolt': {
                'readme_patterns': ['bolt.new', 'built with bolt'],
                'api_patterns': ['bolt api', 'bolt endpoint']
            }
        }

    def detect_platform(self, inputs: PlatformDetectorInputs) -> PlatformDetectorOutputs:
        """Detect which platform was used for the project"""
        
        # Scan workspace for files and content
        file_structure: Dict[str, Any] = self._scan_workspace(inputs.workspace_path)
        package_json: Optional[Dict[str, Any]] = self._parse_package_json(inputs.package_json_content, inputs.workspace_path)
        
        scores: Dict[str, float] = {}
        detailed_analysis: Dict[str, Dict[str, List[str]]] = {}
        
        for platform, signatures in self.platform_signatures.items():
            score, analysis = self._calculate_platform_score(
                inputs, signatures, file_structure, package_json
            )
            scores[platform] = score
            detailed_analysis[platform] = analysis
        
        # Find highest scoring platform
        best_platform = max(scores, key=scores.get)
        confidence = scores[best_platform]
        
        if confidence < 0.3:
            best_platform = "unknown"
        
        # Generate platform-specific recommendations
        config = self._get_platform_config(best_platform, file_structure, package_json)
        workflow = self._get_recommended_workflow(best_platform, confidence)
        migrations = self._get_migration_suggestions(best_platform, scores)
        enhancements = self._get_enhancement_opportunities(best_platform, detailed_analysis.get(best_platform, {}))
        
        return PlatformDetectorOutputs(
            detected_platform=best_platform,
            confidence_score=confidence,
            platform_specific_config=config,
            recommended_workflow=workflow,
            migration_suggestions=migrations,
            enhancement_opportunities=enhancements
        )

    def _scan_workspace(self, workspace_path: str) -> Dict[str, Any]:
        """Scan workspace for files and structure"""
        structure: Dict[str, Any] = {
            'files': [],
            'folders': [],
            'file_contents': {},
            'total_files': 0
        }
        
        try:
            workspace = Path(workspace_path)
            if not workspace.exists():
                return structure
            
            for item in workspace.rglob('*'):
                if item.is_file():
                    relative_path = item.relative_to(workspace)
                    structure['files'].append(str(relative_path))
                    structure['total_files'] += 1
                    
                    # Read content of key files
                    if item.name in ['.replit', 'README.md', 'Dockerfile', '.env.example']:
                        try:
                            with open(item, 'r', encoding='utf-8') as f:
                                structure['file_contents'][str(relative_path)] = f.read()
                        except:
                            pass
                            
                elif item.is_dir():
                    structure['folders'].append(str(item.relative_to(workspace)))
        
        except Exception as e:
            print(f"Error scanning workspace: {e}")
        
        return structure

    def _parse_package_json(self, content: Optional[str], workspace_path: str) -> Optional[Dict[str, Any]]:
        """Parse package.json content"""
        if content:
            try:
                return json.loads(content)
            except:
                pass
        
        # Try to read from workspace
        try:
            package_path = Path(workspace_path) / 'package.json'
            if package_path.exists():
                with open(package_path, 'r', encoding='utf-8') as f:
                    return json.loads(f.read())
        except:
            pass
        
        return None

    def _calculate_platform_score(self, inputs: PlatformDetectorInputs, signatures: Dict[str, Any], 
                                 file_structure: Dict[str, Any], package_json: Optional[Dict[str, Any]]) -> Tuple[float, Dict[str, List[str]]]:
        """Calculate confidence score for a platform with detailed analysis"""
        score = 0.0
        analysis: Dict[str, List[str]] = {
            'file_matches': [],
            'script_matches': [],
            'dependency_matches': [],
            'commit_matches': [],
            'content_matches': [],
            'advanced_matches': []
        }
        
        # Check for signature files (high weight)
        for file_name in signatures.get('files', []):
            if any(file_name in f for f in file_structure.get('files', [])):
                score += 0.4
                analysis['file_matches'].append(file_name)
        
        # Check folder patterns
        for folder_pattern in signatures.get('folder_patterns', []):
            if any(folder_pattern in f for f in file_structure.get('folders', [])):
                score += 0.2
                analysis['file_matches'].append(f"folder: {folder_pattern}")
        
        # Check commit messages (medium weight)
        for commit in inputs.commit_messages:
            for pattern in signatures.get('commit_patterns', []):
                if pattern.lower() in commit.lower():
                    score += 0.15
                    analysis['commit_matches'].append(pattern)
        
        # Check package.json (medium weight)
        if package_json:
            # Check dependencies
            all_deps = {**package_json.get('dependencies', {}), 
                       **package_json.get('devDependencies', {})}
            
            for dep in signatures.get('dependencies', []):
                if dep in all_deps:
                    score += 0.25
                    analysis['dependency_matches'].append(dep)
            
            # Check scripts
            scripts = package_json.get('scripts', {})
            for script in signatures.get('package_scripts', []):
                if script in scripts or any(script in s for s in scripts.values()):
                    score += 0.15
                    analysis['script_matches'].append(script)
        
        # Check file contents (medium weight)
        for file_path, content in file_structure.get('file_contents', {}).items():
            for pattern in signatures.get('content_patterns', []):
                if pattern.lower() in content.lower():
                    score += 0.1
                    analysis['content_matches'].append(f"{pattern} in {file_path}")
        
        # Advanced pattern detection
        platform = [p for p, s in self.platform_signatures.items() if s == signatures][0]
        if platform in self.advanced_patterns:
            advanced_score = self._check_advanced_patterns(
                platform, file_structure, package_json
            )
            score += advanced_score
            if advanced_score > 0:
                analysis['advanced_matches'].append(f"Advanced patterns: +{advanced_score:.2f}")
        
        return min(score, 1.0), analysis

    def _check_advanced_patterns(self, platform: str, file_structure: Dict[str, Any], 
                                package_json: Optional[Dict[str, Any]]) -> float:
        """Check advanced platform-specific patterns"""
        advanced = self.advanced_patterns.get(platform, {})
        score = 0.0
        
        # Check README patterns
        readme_content = ""
        for file_path, content in file_structure.get('file_contents', {}).items():
            if 'readme' in file_path.lower():
                readme_content += content.lower()
        
        for pattern in advanced.get('readme_patterns', []):
            if pattern in readme_content:
                score += 0.1
        
        # Check environment patterns
        for file_path, content in file_structure.get('file_contents', {}).items():
            if '.env' in file_path:
                for pattern in advanced.get('env_patterns', []):
                    if pattern in content:
                        score += 0.15
        
        return score

    def _get_platform_config(self, platform: str, file_structure: Dict[str, Any], 
                           package_json: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Get platform-specific configuration"""
        
        base_config: Dict[str, Any] = {
            'detected_files': [],
            'missing_files': [],
            'recommended_structure': {},
            'deployment_options': []
        }
        
        if platform == "replit":
            base_config.update({
                'runtime_detected': self._detect_replit_runtime(file_structure),
                'replit_config': self._extract_replit_config(file_structure),
                'deployment_options': ['replit-hosting', 'github-export', 'azure-app-service'],
                'recommended_structure': {
                    'main_file': 'main.py' if 'main.py' in file_structure.get('files', []) else 'index.js',
                    'config_file': '.replit',
                    'dependencies_file': 'requirements.txt' if any('py' in f for f in file_structure.get('files', [])) else 'package.json'
                }
            })
        
        elif platform == "lovable":
            base_config.update({
                'framework_detected': 'react',
                'typescript_enabled': self._check_typescript_usage(file_structure),
                'deployment_options': ['vercel', 'netlify', 'azure-static-web-apps'],
                'recommended_structure': {
                    'components_dir': 'src/components',
                    'pages_dir': 'src/pages',
                    'config_file': 'lovable.config.js'
                }
            })
        
        elif platform == "bolt":
            base_config.update({
                'fullstack_detected': True,
                'database_type': self._detect_database_type(file_structure, package_json),
                'deployment_options': ['vercel', 'railway', 'azure-container-instances'],
                'recommended_structure': {
                    'api_dir': 'api' if 'api' in file_structure.get('folders', []) else 'server',
                    'client_dir': 'client' if 'client' in file_structure.get('folders', []) else 'src',
                    'config_file': 'bolt.config.json'
                }
            })
        
        return base_config

    def _get_recommended_workflow(self, platform: str, confidence: float) -> str:
        """Get recommended AutoPR workflow for the platform"""
        
        if confidence < 0.5:
            return "phase2_generic_enhancement"
        
        workflow_map: Dict[str, str] = {
            'replit': 'phase2_replit_to_production',
            'lovable': 'phase2_lovable_enhancement', 
            'bolt': 'phase2_bolt_fullstack_deploy',
            'same': 'phase2_same_customization',
            'emergent': 'phase2_emergent_devops',
            'unknown': 'phase2_platform_detection_required'
        }
        
        return workflow_map.get(platform, 'phase2_generic_enhancement')

    def _get_migration_suggestions(self, platform: str, all_scores: Dict[str, float]) -> List[str]:
        """Get suggestions for migrating to other platforms"""
        suggestions: List[str] = []
        
        # Sort platforms by score (excluding the detected one)
        other_platforms: Dict[str, float] = {k: v for k, v in all_scores.items() if k != platform and v > 0.1}
        sorted_platforms: List[Tuple[str, float]] = sorted(other_platforms.items(), key=lambda x: x[1], reverse=True)
        
        migration_benefits: Dict[str, str] = {
            'replit': 'Better collaboration and instant deployment',
            'lovable': 'Enhanced React/TypeScript development experience',
            'bolt': 'Full-stack development with integrated database',
            'same': 'Faster development using proven patterns',
            'emergent': 'Better DevOps automation and deployment'
        }
        
        for alt_platform, score in sorted_platforms[:3]:
            if score > 0.2:
                benefit = migration_benefits.get(alt_platform, 'Platform-specific benefits')
                suggestions.append(f"Consider migrating to {alt_platform}: {benefit} (compatibility: {score:.1%})")
        
        return suggestions

    def _get_enhancement_opportunities(self, platform: str, analysis: Dict[str, List[str]]) -> List[str]:
        """Get opportunities to enhance the current platform setup"""
        opportunities: List[str] = []
        
        if platform == "replit":
            opportunities.extend([
                "Add production-ready package.json scripts",
                "Implement proper environment variable management",
                "Add Dockerfile for containerized deployment",
                "Setup GitHub Actions for automated deployment"
            ])
        
        elif platform == "lovable":
            opportunities.extend([
                "Add comprehensive TypeScript configurations",
                "Implement component testing with React Testing Library",
                "Setup Storybook for component documentation",
                "Add accessibility testing automation"
            ])
        
        elif platform == "bolt":
            opportunities.extend([
                "Add database migration management",
                "Implement API documentation with OpenAPI",
                "Setup monitoring and logging",
                "Add comprehensive E2E testing"
            ])
        
        # Add generic opportunities based on missing files
        if not analysis.get('file_matches'):
            opportunities.append("Add platform-specific configuration files")
        
        if not analysis.get('dependency_matches'):
            opportunities.append("Update dependencies to use platform-specific packages")
        
        return opportunities

    def _detect_replit_runtime(self, file_structure: Dict[str, Any]) -> str:
        """Detect Replit runtime from file structure"""
        files = file_structure.get('files', [])
        
        if any(f.endswith('.py') for f in files):
            return 'python'
        elif any(f.endswith('.js') or f.endswith('.ts') for f in files):
            return 'nodejs'
        elif any(f.endswith('.java') for f in files):
            return 'java'
        elif any(f.endswith('.cpp') or f.endswith('.c') for f in files):
            return 'cpp'
        else:
            return 'unknown'

    def _extract_replit_config(self, file_structure: Dict[str, Any]) -> Dict[str, Any]:
        """Extract Replit configuration from .replit file"""
        replit_content = file_structure.get('file_contents', {}).get('.replit', '')
        
        config: Dict[str, Any] = {}
        if replit_content:
            # Parse basic .replit configuration
            lines = replit_content.split('\n')
            for line in lines:
                if '=' in line:
                    key, value = line.split('=', 1)
                    config[key.strip()] = value.strip().strip('"')
        
        return config

    def _check_typescript_usage(self, file_structure: Dict[str, Any]) -> bool:
        """Check if TypeScript is being used"""
        files = file_structure.get('files', [])
        return any(f.endswith('.ts') or f.endswith('.tsx') for f in files)

    def _detect_database_type(self, file_structure: Dict[str, Any], package_json: Optional[Dict[str, Any]]) -> str:
        """Detect database type from dependencies and files"""
        if not package_json:
            return 'unknown'
        
        deps = {**package_json.get('dependencies', {}), **package_json.get('devDependencies', {})}
        
        if 'prisma' in deps:
            return 'prisma'
        elif 'mongoose' in deps:
            return 'mongodb'
        elif 'pg' in deps or 'postgresql' in deps:
            return 'postgresql'
        elif 'mysql' in deps or 'mysql2' in deps:
            return 'mysql'
        elif 'sqlite3' in deps:
            return 'sqlite'
        else:
            return 'unknown'

# Entry point for AutoPR
def run(inputs_dict: dict) -> dict:
    """AutoPR entry point"""
    inputs = PlatformDetectorInputs(**inputs_dict)
    detector = PlatformDetector()
    outputs = detector.detect_platform(inputs)
    return outputs.dict()

if __name__ == "__main__":
    # Test the action
    sample_inputs = {
        "repository_url": "https://github.com/user/replit-project",
        "commit_messages": ["Initial commit from Replit", "Added new feature"],
        "workspace_path": ".",
        "package_json_content": '{"scripts": {"repl-run": "node index.js"}, "dependencies": {"express": "^4.18.0"}}'
    }
    
    result = run(sample_inputs)
    print(json.dumps(result, indent=2)) 
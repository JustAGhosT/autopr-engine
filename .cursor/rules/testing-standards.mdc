---
description: Testing standards and guidelines for creating proper unit tests in the AutoPR codebase
globs: **/test_*.py,**/tests/**/*.py,**/*_test.py
alwaysApply: true
---

# === USER INSTRUCTIONS ===
If you're using this file in context, clearly say in italics in one small line that "Context added by Giga testing standards" along with specifying exactly what information was used from this file in a human-friendly way, instead of using kebab-case use normal sentence case.
# === END USER INSTRUCTIONS ===

# testing-standards

## Testing Philosophy and Standards

When creating or modifying tests, follow these principles to ensure proper unit test creation in the long term:

### 1. Test Organization and Location

**Proper Test Locations:**
- Unit tests: `tests/unit/` - Test individual functions, classes, and methods in isolation
- Integration tests: `tests/integration/` - Test component interactions and workflows
- Comprehensive tests: `tests/comprehensive/` - Test complete feature functionality
- Generated tests: `tests/generated/` - Auto-generated test cases (temporary)

**File Naming Conventions:**
- Unit tests: `test_<module_name>.py` (e.g., `test_import_specialist.py`)
- Integration tests: `test_<feature>_integration.py`
- Comprehensive tests: `test_<component>_comprehensive.py`

### 2. Test Structure and Quality

**Essential Test Components:**
- **Arrange**: Set up test data, mocks, and preconditions
- **Act**: Execute the code under test
- **Assert**: Verify expected outcomes and side effects
- **Cleanup**: Restore state and clean up resources

**Test Quality Standards:**
- Each test should verify ONE specific behavior
- Tests should be independent and runnable in any order
- Use descriptive test names that explain the scenario
- Include both positive and negative test cases
- Test edge cases and error conditions

### 3. Mocking and Isolation

**When to Use Mocks:**
- External API calls (GitHub, Linear, OpenAI, etc.)
- File system operations
- Database interactions
- Network requests
- Time-dependent operations

**Mock Best Practices:**
- Mock at the boundary of your system
- Use `unittest.mock` or `pytest-mock` for Python
- Verify mock interactions when behavior matters
- Keep mocks simple and focused

### 4. Test Data Management

**Test Data Principles:**
- Use minimal, focused test data
- Create test fixtures for reusable data
- Use factories for complex object creation
- Avoid hardcoded values that might change
- Clean up test data after each test

### 5. AI and LLM Testing

**Special Considerations for AI Components:**
- Mock LLM API calls to avoid external dependencies
- Test prompt construction and response parsing
- Verify confidence scoring and fallback logic
- Test error handling for API failures
- Use deterministic test data for consistent results

### 6. Performance and Integration Testing

**Performance Testing:**
- Test response times for critical paths
- Verify memory usage doesn't grow unbounded
- Test with realistic data volumes
- Monitor for performance regressions

**Integration Testing:**
- Test component interactions end-to-end
- Verify data flow between systems
- Test error propagation and recovery
- Validate configuration and environment setup

### 7. Test Maintenance and Evolution

**Long-term Test Health:**
- Refactor tests when code changes significantly
- Remove obsolete tests promptly
- Update tests when requirements change
- Maintain test documentation and comments
- Regular test review and cleanup

**Test Coverage Goals:**
- Aim for 80%+ code coverage on critical paths
- Focus on business logic and error handling
- Don't test trivial getters/setters
- Prioritize testing complex algorithms and workflows

### 8. Specific AutoPR Testing Patterns

**AI Linting Fixer Tests:**
- Test issue detection accuracy
- Verify fix application logic
- Test provider fallback mechanisms
- Validate confidence scoring algorithms

**Queue Management Tests:**
- Test atomic operations and race conditions
- Verify worker ID handling
- Test queue persistence and recovery
- Validate priority and ordering logic

**Platform Detection Tests:**
- Test detection confidence scoring
- Verify platform classification logic
- Test enhancement recommendation generation
- Validate configuration parsing

### 9. Test Execution and CI/CD

**Test Execution Standards:**
- All tests must pass before merging
- Use pytest for Python test execution
- Include test results in CI/CD pipelines
- Generate coverage reports for review
- Run tests in parallel when possible

**Continuous Integration:**
- Fast feedback loop for developers
- Comprehensive test suite for releases
- Performance regression detection
- Security and quality gate enforcement

## Implementation Guidelines

When creating tests:

1. **Start with the most critical functionality** - Test core business logic first
2. **Use the existing test structure** - Follow established patterns in the codebase
3. **Write tests that will catch real bugs** - Focus on scenarios that could break in production
4. **Make tests maintainable** - Clear, readable, and easy to modify
5. **Document test intentions** - Explain why tests exist and what they verify

## Anti-Patterns to Avoid

- **Don't test implementation details** - Test behavior, not internal structure
- **Don't create brittle tests** - Avoid tests that break with minor code changes
- **Don't skip error testing** - Always test error conditions and edge cases
- **Don't create slow tests** - Optimize for speed while maintaining quality
- **Don't ignore flaky tests** - Fix or remove unreliable tests immediately

This testing framework ensures that AutoPR maintains high code quality and reliability through comprehensive, well-structured test coverage that evolves with the codebase.

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga testing standards" along with specifying exactly what information was used from this file in a human-friendly way, instead of using kebab-case use normal sentence case.
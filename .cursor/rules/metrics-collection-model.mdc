---
description: Defines the metrics collection and monitoring system for tracking AutoPR's AI code review performance and quality metrics
globs: **/metrics_collector.py,**/metrics.py,**/models.py,**/*metrics*.py,**/evaluation/**
alwaysApply: false
---

# === USER INSTRUCTIONS ===
If you're using this file in context, clearly say in italics in one small line that "Context added by Giga metrics-collection-model" along with specifying exactly what information was used from this file in a human-friendly way, instead of using kebab-case use normal sentence case.
# === END USER INSTRUCTIONS ===

# metrics-collection-model

Core Metrics Collection System:

1. Success Rate Tracking
- Custom weighted metric calculation incorporating:
  * Fix success rate (25%)
  * User satisfaction (20%)
  * Uptime metrics (15%)
  * Test pass rates (15%) 
  * Code quality scores (10%)
  * Security metrics (10%)
  * Error rates (-5% penalty)

2. Performance Analytics Framework
- Specialized measurement system for:
  * Response timing per model/provider
  * Token usage efficiency 
  * CPU/Memory consumption patterns
  * API cost optimization tracking
  * Queue processing efficiency

3. Quality Assessment Pipeline
- Domain-specific quality metrics:
  * Code fix accuracy scores
  * Documentation coverage rates
  * Test coverage measurements
  * Security compliance checks
  * Performance impact analysis

4. Trend Analysis System
- Custom trend classification logic:
  * "improving": >5% positive change
  * "declining": >5% negative change
  * "stable": changes within Â±5%
  * Uses 7-day moving averages

5. Automated Recommendation Engine
- Context-aware suggestions based on:
  * Fix success rate thresholds (<80%)
  * User satisfaction scores (<3.5/5)
  * Response time metrics (>5s)
  * Error frequency patterns (>10%)

Core Implementation Files:
- autopr/evaluation/metrics_collector.py
- autopr/actions/ai_linting_fixer/metrics.py
- autopr/quality/metrics_collector.py

The system uniquely combines AI performance metrics with code quality measurements to provide comprehensive insights into automated PR management effectiveness.

Importance Score: 95 - Critical for measuring and optimizing core business operations

## Testing Requirements for Metrics Collection

When working with metrics collection components, ensure proper testing by:

1. **Unit Testing Metrics Calculations:**
   - Test weighted metric calculations with known inputs
   - Verify success rate algorithms with edge cases
   - Test trend analysis logic with various data patterns
   - Validate recommendation engine thresholds

2. **Integration Testing:**
   - Test metrics aggregation across multiple sources
   - Verify data pipeline integrity and transformations
   - Test performance analytics with realistic workloads
   - Validate quality assessment pipeline end-to-end

3. **Data Validation Testing:**
   - Test with malformed or missing data
   - Verify handling of zero values and edge cases
   - Test data consistency across different collection points
   - Validate error handling for data corruption

4. **Performance Testing:**
   - Test metrics collection under high load
   - Verify memory usage for large datasets
   - Test query performance for trend analysis
   - Monitor for performance regressions

**Test Location Guidelines:**
- Unit tests: `tests/unit/test_metrics_*.py`
- Integration tests: `tests/integration/test_metrics_integration.py`
- Comprehensive tests: `tests/comprehensive/test_metrics_collector_comprehensive.py`

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga metrics-collection-model" along with specifying exactly what information was used from this file in a human-friendly way, instead of using kebab-case use normal sentence case.
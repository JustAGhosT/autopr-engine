# =============================================================================
# AI Linting Fixer - Workflow Integration Configuration
# =============================================================================
#
# This configuration file demonstrates how to integrate the AI linting system
# with various workflow orchestration platforms and execution environments.
#
# Author: AutoPR AI Systems
# Updated: 2025-01-29

# =============================================================================
# STANDALONE CONFIGURATION
# =============================================================================
standalone:
  description: "Direct execution without orchestration"
  enabled: true

  defaults:
    max_fixes_per_run: 20
    max_workers: 4
    provider: "azure_openai"
    model: "gpt-4.1"
    fix_types: ["E501", "F401", "F841", "F821", "E722"]

  execution:
    timeout_seconds: 300
    retry_config:
      max_retries: 3
      retry_delay: 1.0
      exponential_backoff: true

  resource_requirements:
    cpu_cores: 4
    memory_mb: 1024
    api_quota: 100

# =============================================================================
# TEMPORAL.IO ORCHESTRATION
# =============================================================================
temporal:
  description: "Enterprise workflow orchestration with Temporal.io"
  enabled: false # Enable when Temporal is available

  workflow_config:
    task_queue: "ai-linting-queue"
    workflow_execution_timeout: "10m"
    workflow_task_timeout: "1m"

  activity_config:
    start_to_close_timeout: "5m"
    schedule_to_start_timeout: "1m"
    heartbeat_timeout: "30s"
    retry_policy:
      initial_interval: "1s"
      backoff_coefficient: 2.0
      maximum_interval: "10s"
      maximum_attempts: 3

  # Integration settings
  integration:
    workflow_class: "CodeQualityWorkflow"
    activity_name: "ai_linting_activity"
    dependencies: ["repo_checkout", "dependency_install"]
    triggers: ["run_tests", "security_scan"]

# =============================================================================
# CELERY DISTRIBUTED PROCESSING
# =============================================================================
celery:
  description: "Distributed task processing with Celery"
  enabled: false # Enable when Redis/RabbitMQ is available

  broker_config:
    broker_url: "redis://localhost:6379/0"
    result_backend: "redis://localhost:6379/0"

  task_config:
    task_name: "ai_linting_task"
    task_serializer: "json"
    result_serializer: "json"
    accept_content: ["json"]

    # Task execution settings
    task_time_limit: 600 # 10 minutes
    task_soft_time_limit: 540 # 9 minutes
    task_acks_late: true
    worker_prefetch_multiplier: 1

    # Retry configuration
    autoretry_for: ["ConnectionError", "TimeoutError"]
    retry_kwargs:
      max_retries: 3
      countdown: 60

  # Worker scaling
  worker_config:
    concurrency: 4
    max_tasks_per_child: 1000
    worker_hijack_root_logger: false

  # Integration with other tasks
  integration:
    depends_on: ["prepare_environment"]
    triggers: ["notify_completion", "update_metrics"]

# =============================================================================
# PREFECT DATA PIPELINE
# =============================================================================
prefect:
  description: "Modern data pipeline orchestration with Prefect"
  enabled: false # Enable when Prefect is available

  flow_config:
    flow_name: "code_quality_pipeline"
    description: "AI-powered code quality improvement pipeline"
    tags: ["code-quality", "ai", "linting"]

  task_config:
    task_name: "ai_linting_task"
    retries: 3
    retry_delay_seconds: 30
    timeout_seconds: 300

  # Parallel execution strategy
  execution_strategy:
    # Process different issue types in parallel
    parallel_fix_types:
      style_fixes: ["F541", "E741"]
      import_fixes: ["F401", "F811"]
      length_fixes: ["E501"]
      variable_fixes: ["F841", "F821"]
      exception_fixes: ["E722", "B001"]

    # Resource allocation per parallel task
    resource_allocation:
      max_workers_per_task: 2
      memory_limit_mb: 512

  # Integration with data stores
  integration:
    result_storage: "local" # or "s3", "gcs", "azure"
    artifact_storage: "local"
    logging_level: "INFO"

# =============================================================================
# FASTAPI MICROSERVICE
# =============================================================================
fastapi:
  description: "Microservice API with FastAPI"
  enabled: true # Always available for API access

  api_config:
    title: "AI Linting Service"
    description: "AutoPR AI-powered code linting microservice"
    version: "1.0.0"
    host: "0.0.0.0"
    port: 8000

  endpoint_config:
    base_path: "/api/v1"
    endpoints:
      lint: "/lint"
      status: "/status"
      metrics: "/metrics"
      health: "/health"

  # Rate limiting and security
  security:
    rate_limit: "100/minute"
    require_api_key: false # Set to true in production
    cors_origins: ["*"] # Restrict in production

  # Async processing
  async_config:
    background_tasks: true
    callback_support: true
    webhook_timeout: 30

  integration:
    metrics_export: "prometheus"
    logging_format: "json"
    health_checks: ["llm_provider", "database", "file_system"]

# =============================================================================
# GITHUB ACTIONS CI/CD
# =============================================================================
github_actions:
  description: "GitHub Actions CI/CD integration"
  enabled: true

  workflow_config:
    name: "AI Code Quality"
    triggers: ["pull_request", "push"]
    branches: ["main", "develop"]

  job_config:
    runs_on: "ubuntu-latest"
    timeout_minutes: 15

    steps:
      - name: "Checkout code"
        uses: "actions/checkout@v3"

      - name: "Setup Python"
        uses: "actions/setup-python@v4"
        with:
          python_version: "3.11"

      - name: "Install dependencies"
        run: "pip install -r requirements-dev.txt"

      - name: "Run AI Linting"
        run: "python tools/ai_lint_fixer.py --max-fixes=10 --verbose"
        env:
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}

      - name: "Commit fixes"
        if: "success()"
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -A
          git diff --staged --quiet || git commit -m "ðŸ¤– AI: Auto-fix linting issues"
          git push

# =============================================================================
# AIRFLOW DATA PIPELINE
# =============================================================================
airflow:
  description: "Apache Airflow data pipeline integration"
  enabled: false # Enable when Airflow is available

  dag_config:
    dag_id: "ai_code_quality_pipeline"
    description: "AI-powered code quality improvement pipeline"
    schedule_interval: "@daily"
    catchup: false
    max_active_runs: 1

  task_config:
    task_id: "ai_linting_task"
    python_callable: "ai_linting_fixer"
    pool: "ai_processing_pool"

    # Resource requirements
    resources:
      cpus: 2
      memory: "2GB"

  # Dependencies and triggers
  pipeline_structure:
    upstream_tasks: ["checkout_repo", "install_deps"]
    downstream_tasks: ["run_tests", "deploy_if_passing"]

  # Integration with Airflow features
  integration:
    use_xcom: true
    task_groups: ["code_quality"]
    sensors: ["file_sensor"]

# =============================================================================
# KUBERNETES JOB ORCHESTRATION
# =============================================================================
kubernetes:
  description: "Kubernetes job-based orchestration"
  enabled: false # Enable when K8s is available

  job_config:
    name: "ai-linting-job"
    namespace: "autopr"
    restart_policy: "OnFailure"
    backoff_limit: 3
    active_deadline_seconds: 600

  pod_config:
    image: "autopr/ai-linting:latest"
    resources:
      requests:
        cpu: "1000m"
        memory: "1Gi"
      limits:
        cpu: "2000m"
        memory: "2Gi"

  # Persistent storage
  storage:
    volume_mounts:
      - name: "code-volume"
        mount_path: "/code"
      - name: "cache-volume"
        mount_path: "/cache"

  # Integration with K8s ecosystem
  integration:
    service_account: "ai-linting-sa"
    config_maps: ["ai-linting-config"]
    secrets: ["ai-api-keys"]

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================
monitoring:
  description: "Monitoring and observability configuration"

  metrics:
    enabled: true
    provider: "prometheus" # or "datadog", "newrelic"

    # Key metrics to track
    tracked_metrics:
      - "ai_linting_issues_fixed_total"
      - "ai_linting_files_processed_total"
      - "ai_linting_duration_seconds"
      - "ai_linting_api_calls_total"
      - "ai_linting_confidence_score"
      - "ai_linting_success_rate"

  logging:
    enabled: true
    level: "INFO"
    format: "json"

    # Log destinations
    destinations:
      - "stdout" # For containerized environments
      - "file:/var/log/ai_linting.log" # For persistent logging

  tracing:
    enabled: false # Enable for distributed tracing
    provider: "jaeger" # or "zipkin", "datadog"

  alerting:
    enabled: true

    # Alert conditions
    alerts:
      - name: "High failure rate"
        condition: "success_rate < 0.8"
        severity: "warning"

      - name: "API quota exceeded"
        condition: "api_calls_per_hour > 1000"
        severity: "critical"

      - name: "Long processing time"
        condition: "duration_seconds > 300"
        severity: "warning"

# =============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# =============================================================================
environments:
  development:
    max_fixes_per_run: 5
    max_workers: 2
    provider: "azure_openai"
    verbose: true

  staging:
    max_fixes_per_run: 15
    max_workers: 4
    provider: "azure_openai"
    monitoring:
      metrics:
        enabled: true

  production:
    max_fixes_per_run: 30
    max_workers: 8
    provider: "azure_openai"

    # Enhanced monitoring in production
    monitoring:
      metrics:
        enabled: true
      logging:
        level: "WARNING"
      alerting:
        enabled: true

    # Production security
    security:
      require_api_key: true
      rate_limiting: true
      audit_logging: true
